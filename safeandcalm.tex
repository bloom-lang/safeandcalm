\documentclass{sig-alternate}

\usepackage[usenames, dvipsnames]{color}
\usepackage{times}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{url}
%\usepackage{amsmath, amssymb}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{comment}
\usepackage{alltt}
\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{color}
\usepackage{listings}
\lstset{ %
basicstyle=\ttfamily\scriptsize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\ttfamily,      % the size of the fonts that are used for the line-numbers
%aboveskip=0pt,
%belowskip=0pt,
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
%numbersep=10pt,                  % how far the line-numbers are from the code
breakindent=0pt,
firstnumber=0,
%backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=leftline,
tabsize=2,  		% sets default tabsize to 2 spaces
captionpos=b,   		% sets the caption-position to bottom
breaklines=false,    	% sets automatic line breaking
breakatwhitespace=true,    % sets if automatic breaks should only happen at whitespace
%escapeinside={\%}{)}          % if you want to add a comment within your code
columns=fixed,
basewidth=0.52em,
% are you fucking kidding me lstlistings?  who puts the line numbers outside the margin?
xleftmargin=6mm,
xrightmargin=-6mm,
numberblanklines=false,
language=Ruby,
morekeywords={declare,table,scratch,channel,interface,periodic}
}
\lstset{escapeinside={(*}{*)}}
%\renewcommand*\thelstnumber{\the\value{lstnumber}:}


%\linespread{0.975}

\newcommand{\jmh}[1]{{\textcolor{red}{#1 -- jmh}}}
\newcommand{\paa}[1]{{\textcolor{blue}{#1 -- paa}}}
\newcommand{\pb}[1]{{\textcolor{green}{#1 -- kc}}}
\newcommand{\smallurl}[1]{{\small \url{#1}}}

\begin{document}
\title{CALM and Safe: Understanding When Computers Should Coordinate}

\numberofauthors{1}
\author{
Authors
}

\maketitle

\begin{abstract}
We present a foundation for applied researchers to understand recent research on the role of coordination in computer systems.  We discuss the mechanism-independent essence of coordination, and illustrate how technology trends make coordination increasingly problematic in modern systems at many scales.  We describe the CALM Theorem~\cite{}, a foundation for capturing precisely when coordination is needed and when it can be avoided.  We extend the CALM Theorem to apply to a safety properties, and discuss design patterns that have emerged for avoiding coordination in this context.  

In addition to introducing recent formal work in an applied context, we present two new results: a trend analysis of the cost of coordination across multiple scales, and a unification of the CALM Theorem and Invariant Confluence~\cite{}.  
\end{abstract}

\section{Introduction}
There has been substantial research in recent years on the computational role of \emph{coordination}: techniques that are used to enforce order among cooperating processes.  The design of mechanisms to achieve coordination goes back to the earliest days of computer and database systems~\cite{grayreuter,seminalosbook}, including shared-memory mechanisms like semaphores~\cite{semaphores} and locks~\cite{2pl}, and message passing protocols for commit~\cite{2pl,3pl} and consensus~\cite{paxos,virtualsynchrony}.  An enormous variety of variations and enhancements to these techniques have been developed over the decades since these issues were first codified.

The advances of Moore's Law and the attendant rise of global-scale computing has led to a situation where coordination protocols are a significant bottleneck at all levels of the memory hierarchy; we explore this topic in detail in Section~\ref{sec:costs}.  As a result of the painful cost...

\section{What is Coordination?}
Informally: a process engages in coordination when it waits for another process, even though it has all the data it needs to proceed.  Note how the essence of this definition distinguishes coordination from data dependencies.  A coordination-free program allows data to ``flow'' freely, and processes are free to proceed as soon as they have the data they need.  The role of coordination is to enforce correctness when this free flow of data can lead to non-deterministic outcomes.

To formalize coordination, then, we need a model that captures dataflow in a formal manner.  Database query languages are one such model. The design of Bloom led to the CALM Conjecture.  Which in turn led to the following formal characterization by Ameloot, et al.  

Gloss on Ameloot's model.

\section{The Unbearable Cost of Coordination}
\label{sec:costs}
\emph{It's really quite bad in a lot of circumstances.  And it's going to get worse.}
\subsection{Coordination Ruins Everything Around Me}
\emph{Some history here.  First driven by distributed systems and speed of light: including LADIS quotes, CAP theorem.  Also emerging in high-performance systems today: Abadi, Hekaton, Kohler/Kaashoek.  Even in new places like ML (Hogwild!, etc.)  A shred of boilerplate below.}

This issue was most memorably illustrated by the CAP Theorem, which observed that very long delays among coordinating processes (i.e., ``partitions'') might require programmers to give up on familiar models of memory ``consistency'', specifically linearizability.  As the CAP Theorem became folklore in the developer community, there was an increasing desire 


\emph{A bottleneck analysis goes here.  The goal is a model that explains when coordination is expensive relative to latency and/or throughput. The model can start with distributed systems, but should connect to on-chip performance trends as well.  Maybe solicit input from Patterson?}

\section{Why Coordinate?}
First, hammer home the point that this is a question of tractability.  Start with a ``strawman'' scenario: suppose we are forbidden from using coordination, as suggested by Hamilton and espoused by the NoSQL crowd.  \textbf{Question 1:} What ``correct'' programs can we write in this fashion, and what programs are impossible?  If we can answer this question, we can move on to the constructive question:  \textbf{Question 2:} If a program is expressible without coordination, can I find an implementation, and will it be faster than a more straightforward solution that uses coordination?  We address the first question in this section, and turn to the second in Section~\ref{sec:patterns}.

\subsection{Confluence as a Touchstone}
ACID, CAP and CALM all spoke of ``Consistency''.  They had three different definitions.  The word is clearly to be avoided.

What we want to focus on is the way that uncontrolled concurrency can affect program semantics.  Hence Confluence (the real C in CALM) is a good touchstone.

\subsection{The CALM Theorem and Extensions}
Intuition from dataflow and streaming: what kinds of operations need to wait for something other than data?  

Enhance to ``Focused CALM'' analysis w.r.t.\ a subset of the tables in a program.  Trivial case: focus on a monotone set of rules/tables, but there exists a totally disconnected non-monotone set of rules/tables.  All other cases are subtler variants of this.

\subsection{Ensuring Safety via Invariants}
The CALM theorem provides a formal foundation for ensuring confluence of outcomes.  Bailis, et al.~considered the \emph{safety} question of whether states ``along the way'' to a final result could be given guarantees as well.  This is captured via the notion of \emph{Invariant Confluence}: the assurance that non-deterministic choices always lead to states where desired invariants hold.

Invariant Confluence is captured naturally in the framework of the CALM Theorem: after all, invariants are, by nature, logic statements.

Consider an invariant I(D) that must hold over any database state D.  Using DeMorgan's Law we can write its negation Not\_I, and then introduce a definition of safety as follows:

\begin{verbatim}
  failure <= Not_I(D)
  safety <= safety {|s| [true] if failure.is_empty?}
\end{verbatim}

\texttt{failure} and \texttt{safety} are not keywords in Bloom; they are just tables serving as boolean variable: empty when false (i.e., \emph{propositions}). We capture the semantics of \texttt{safety} as a program rewrite, by adding the predicate \texttt{and safety.exists?} to every rule.  Upon execution of such a Bloom program, it terminates with a non-empty failure predicate whenever invariant failure is detected.  Call this the \emph{invariant rewrite of P} or $P^I$.  

What coordination-free programs have Invariant Confluence?   Those $P_I$ that are CALM with respect to $I$. As an example, consider any monotone program $P$, a set of monotone invariants $I$, and the invariant rewrite $P^I$.  Note that the rewritten rules of $P$ depend positively (monotonically) on \texttt{safety}, which in turn depends negatively (non-monotonically) on \texttt{failure}.  Note also that \texttt{failure} depends monotonically on $I$, and transitively may depend on $P$.

We don't care about cases where I(D) is false: i.e. programs that ``start out'' with invariants violated.  We want to ensure that if \texttt{safety} gets into the full state (invariants are satisfied) it stays full.  That is, \texttt{safety} is \emph{monotone} in $P_I$.   Based on the dependency structure of the rewrite, as shown in Figure~\ref{fig:invariant-rewrite}, this is true exactly when \texttt{failure} is \emph{anti-monotone} in $P_I$: if it gets into the empty state it stays empty.  

Consider the case where the invariants $I$ are all monotone--both with respect to $P$ and with respect to each other.  In that case, $\mbox{\texttt{safety}} \cup P \cup I \cup \mbox{\texttt{failure}}$ is monotone. The only non-monotonic dependency is between the propositional predicates \texttt{failure} and \texttt{safety}.  It is easy to show in that case that the entire program is monotone with respect to $P$.

\emph{little lemma here.}

\emph{ is to use more esoteric monotonoicity tests to prove IC for }

\emph{Would be nice to demonstrate Blazes detecting all this automagically!}

Bailis et al. list of invariants:
\begin{itemize}
\item {\bf Confluent:} Per-record Equality/Inequality check (e.g. not null), Uniqueness with choice, FK Insert, FK Cascading Delete, Index/MatView (more FDs), $>$ with increment, $<$ with decrement, CONTAINS.
\item {\bf Non-Confluent:} Uniqueness w. specific value, auto\_increment, FK Delete, non-monotone compare.
\end{itemize}

Of these, 
\begin{itemize}
\item {\bf obviously monotonic:} Per-record Equality/Inequality check (e.g. not null), FK Insert, inequalities with incr/decr, CONTAINS
\item {\bf wrong:} Uniqueness with choice.
\item {\bf to be shown monotone:} FK Cascading Delete and delete from Index/MatView
\item {\bf to be shown non-monotone:} Uniqueness w. specific value, auto\_increment, FK Delete
\item {\bf obviously non-monotone:} no-monotone compare
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{invariant-rewrite.pdf}
\vspace{-10pt}
\caption{A data dependency graph for a program $P$ and invariants $I$ written in negated form.  $P$ depends positively on \texttt{safety} as shown via a blue arrow, which in turn depends negatively on \texttt{failure}, shown via the red arrow.  In general, the negated $Not_I$ predicates may depend negatively on $P$, hence the yellow arrow.}
\label{fig:invariant-rewrite}
\vspace{-2pt}
\end{figure}

\section{Design Patterns for Coordination Avoidance}
\label{sec:patterns}

\subsection{Design for Commmutativity}
CRDTS, Kohler/Kaashoek.  Works in simple cases, easy to understand.  Recap intro to Neil's paper.
\subsection{Design for Monotonicity}
How can we compose commutative operators?  

\bibliographystyle{abbrv}
\bibliography{safecalm}

\end{document}
